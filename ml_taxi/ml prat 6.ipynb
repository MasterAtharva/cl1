{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3321914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     --------------------------- ---------- 524.3/721.7 kB 2.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 721.7/721.7 kB 2.0 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\athar\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from gym) (3.0.0)\n",
      "Collecting gym-notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827633 sha256=45953d4da5076394661d4ea948ad4a6a64e5bf4713c1175e5211480ab8c4a616\n",
      "  Stored in directory: c:\\users\\athar\\appdata\\local\\pip\\cache\\wheels\\95\\51\\6c\\9bb05ebbe7c5cb8171dfaa3611f32622ca4658d53f31c79077\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, gym\n",
      "Successfully installed gym-0.26.2 gym-notices-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc0f0ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 26.13\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "# Create the Taxi environment\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "# Initialize Q-table (500 states, 6 possible actions)\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.6  # Discount factor\n",
    "epsilon = 0.1  # Exploration-exploitation tradeoff\n",
    "episodes = 10000  # Number of episodes for training\n",
    "\n",
    "# Training the agent using Q-learning\n",
    "for episode in range(episodes):\n",
    "    # Reset the environment and capture the state\n",
    "    state = env.reset() if isinstance(env.reset(), int) else env.reset()[0]\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Exploration-exploitation tradeoff\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploit learned values\n",
    "\n",
    "        # Take action and observe outcome\n",
    "        result = env.step(action)\n",
    "        if len(result) == 4:  # For older gym versions\n",
    "            next_state, reward, done, info = result\n",
    "        else:  # For newer gym versions (with 'truncated')\n",
    "            next_state, reward, done, truncated, info = result\n",
    "            done = done or truncated  # End the episode if either is True\n",
    "\n",
    "        # Update Q-value using the Q-learning formula\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "\n",
    "        # Q-learning update formula\n",
    "        q_table[state, action] = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "\n",
    "        # Transition to the next state\n",
    "        state = next_state\n",
    "\n",
    "# Testing the trained agent\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes_test = 100\n",
    "\n",
    "for _ in range(episodes_test):\n",
    "    state = env.reset() if isinstance(env.reset(), int) else env.reset()[0]\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])  # Exploit learned values\n",
    "        result = env.step(action)\n",
    "\n",
    "        if len(result) == 4:  # Handle older gym version\n",
    "            next_state, reward, done, info = result\n",
    "        else:  # Handle newer gym version with truncated flag\n",
    "            next_state, reward, done, truncated, info = result  \n",
    "            done = done or truncated\n",
    "\n",
    "        if reward == -10:  # Penalty for illegal pick-up/drop-off\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        state = next_state\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes_test} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes_test}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d323c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some cumulative reward. Unlike supervised learning, where models learn from labeled data, RL works through trial and error, receiving feedback in the form of rewards or penalties based on the outcomes of its actions.\n",
    "Types of Reinforcement Learning:\n",
    "Model-Free RL: The agent learns directly from experience without any knowledge of the environmentâ€™s dynamics.\n",
    "\n",
    "Q-Learning: An off-policy method that uses a Q-value (quality value) table to store the expected future rewards for actions in each state.\n",
    "SARSA: An on-policy method that updates the Q-values based on the action actually taken in the next state.\n",
    "Model-Based RL: The agent builds a model of the environment and uses it to make predictions about the consequences of actions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
